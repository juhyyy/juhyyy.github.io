<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Juhyeong Kang | Deployment Strategy & Applied AI</title>

<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">

<style>
:root {
  --palantir-blue: #003c7a;
  --text-dark: #0f1115;
  --text-gray: #6b7280;
  --border: rgba(0,0,0,0.08);
  --bg-ontology: #f9fbff;
}

body {
  margin: 0;
  padding: 0;
  background: #ffffff;
  font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
  color: var(--text-dark);
  line-height: 1.6;
}

/* NAV */
nav {
  padding: 22px 42px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  border-bottom: 1px solid var(--border);
  background: white;
}

nav .left {
  font-size: 18px;
  font-weight: 600;
}

nav .right a {
  margin-left: 26px;
  text-decoration: none;
  color: var(--text-dark);
  font-size: 15px;
  opacity: 0.9;
}

nav .right a:hover {
  color: var(--palantir-blue);
  opacity: 1;
}

/* LAYOUT */
.wrapper {
  max-width: 860px;
  margin: 0 auto;
  padding: 0 28px;
}

/* HERO */
.hero {
  margin-top: 72px;
  margin-bottom: 72px;
}

.hero-name {
  font-size: 40px;
  font-weight: 600;
  margin-bottom: 6px;
}

.hero-role {
  font-size: 17px;
  font-weight: 500;
  color: var(--palantir-blue);
  margin-bottom: 20px;
}

.hero-summary {
  font-size: 17px;
  color: var(--text-gray);
  max-width: 700px;
}

/* SECTIONS */
.section {
  margin: 80px 0;
}

.section h2 {
  font-size: 26px;
  margin-bottom: 22px;
  font-weight: 600;
}

.section p {
  color: var(--text-gray);
  font-size: 16px;
}

/* ONTOLOGY SECTION */
.ontology-wrapper {
  border: 1px solid var(--border);
  border-radius: 14px;
  padding: 24px 22px 26px 22px;
  background: var(--bg-ontology);
}

.ontology-wrapper p {
  margin-top: 0;
  font-size: 14.5px;
  color: var(--text-gray);
}

.ontology-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
  gap: 18px;
  margin-top: 16px;
}

.ontology-node {
  border-radius: 10px;
  border: 1px solid rgba(0,0,0,0.06);
  background: #ffffff;
  padding: 14px 14px 16px 14px;
}

.ontology-title {
  font-size: 14px;
  font-weight: 600;
  color: var(--palantir-blue);
  margin-bottom: 6px;
  text-transform: uppercase;
  letter-spacing: 0.04em;
}

.ontology-node ul {
  padding-left: 18px;
  margin: 0;
}

.ontology-node li {
  font-size: 13.5px;
  color: var(--text-gray);
  margin-bottom: 4px;
}

/* PROJECT BLOCKS */
.project-block {
  border: 1px solid var(--border);
  padding: 24px 24px 26px 24px;
  border-radius: 12px;
  margin-bottom: 28px;
  transition: 0.2s ease;
  background: #ffffff;
}

.project-block:hover {
  border-color: var(--palantir-blue);
  background: #f9fbff;
}

.project-block h3 {
  font-size: 20px;
  margin-bottom: 8px;
}

.project-meta {
  font-size: 14px;
  color: var(--palantir-blue);
  margin-bottom: 14px;
}

/* PROJECT IMAGE (Í∏∞Î≥∏ Îã®Ïùº Ïù¥ÎØ∏ÏßÄÏö©) */
.project-image {
  width: 100%;
  border-radius: 10px;
  margin-bottom: 16px;
  border: 1px solid rgba(0,0,0,0.06);
  object-fit: cover;
}

/* LISTS */
.project-block ul {
  padding-left: 20px;
  color: var(--text-gray);
  margin: 4px 0 10px 0;
}

.project-block li {
  margin-bottom: 6px;
}

/* FOOTER */
footer {
  margin: 90px 0 50px 0;
  text-align: center;
  color: var(--text-gray);
  font-size: 14px;
}

/* FIGURE GRID (Î©ÄÌã∞ Ïù¥ÎØ∏ÏßÄÏö©) */
.figure-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
  gap: 14px;
  margin-bottom: 18px;
  align-items: start; /* Ï∫°ÏÖòÍ≥º Í∞ÑÍ≤© Ïú†ÏßÄ */
}

.figure-item {
  display: flex;
  flex-direction: column;
}

.figure-item img {
  width: 100%;
  height: 180px;          /* üîπ ÎÜíÏù¥ ÎèôÏùºÌïòÍ≤å Ïú†ÏßÄ (Ï§Ñ ÎßûÏ∂îÍ∏∞Ïö©) */
  object-fit: contain;    /* üîπ Ïù¥ÎØ∏ÏßÄ ÏûêÎ•¥ÏßÄ ÏïäÏùå */
  object-position: center;
  border-radius: 8px;
  border: 1px solid rgba(0,0,0,0.06);
  background: #ffffff;    /* üîπ Ïó¨Î∞± Î∂ÄÎ∂Ñ Ìù∞ÏÉâ Ï±ÑÏö∞Í∏∞ */
}

.figure-caption {
  font-size: 12.5px;
  color: var(--text-gray);
  margin-top: 6px;
  min-height: 32px;       /* üîπ Ï∫°ÏÖò ÎÜíÏù¥ ÌÜµÏùº */
}


</style>
</head>

<body>

<!-- NAV -->
<nav>
  <div class="left">Juhyeong Kang</div>
  <div class="right">
    <a href="#about">About</a>
    <a href="#ontology">Ontology</a>
    <a href="#work">Work</a>
    <a href="mailto:kju325@gmail.com">Contact</a>
    <!-- üîπ assets Í≤ΩÎ°ú Ïò§ÌÉÄ ÏàòÏ†ï -->
    <a href="assets/CV_JuhyeongKang.pdf" target="_blank">Download CV</a>
  </div>
</nav>

<div class="wrapper">

  <!-- HERO -->
  <section class="hero">
    <div class="hero-name">Juhyeong Kang</div>
    <div class="hero-role">Deployment Strategy & Applied AI</div>
    <p class="hero-summary">
      I design data workflows that connect users, models, and operations. 
      My work focuses on turning noisy, fragmented information into structured systems that 
      make decisions clearer and faster in high-stakes environments.
    </p>
  </section>

  <!-- ABOUT -->
  <section id="about" class="section">
    <h2>About</h2>
    <p>
      I work across problem framing, data modeling, and workflow design. 
      From healthcare AI to LLM-based reasoning systems, I focus on how people actually use tools:
      which questions they ask, which signals they trust, and how data needs to be shaped 
      so it becomes reliable context inside their day-to-day decisions.
    </p>
  </section>

  <!-- ONTOLOGY STYLE SECTION -->
  <section id="ontology" class="section">
    <h2>System Ontology</h2>
    <div class="ontology-wrapper">
      <p>
        A simplified view of how my work connects <b>data sources</b>, <b>models</b>, <b>workflows</b>, 
        and <b>operational outcomes</b> ‚Äî similar in spirit to an Ontology, where objects and relationships 
        are explicit rather than implicit.
      </p>

      <div class="ontology-grid">
        <div class="ontology-node">
          <div class="ontology-title">Data Sources</div>
          <ul>
            <li>4D ultrasound frames & medical imaging</li>
            <li>EEG signals (AHI, SpO‚ÇÇ-related)</li>
            <li>Behavioral & dialogue logs (ASD learners)</li>
            <li>Text logs for intent classification</li>
          </ul>
        </div>

        <div class="ontology-node">
          <div class="ontology-title">Model Objects</div>
          <ul>
            <li>LLM + RAG intent engine</li>
            <li>Interaction clusters & learner profiles</li>
            <li>Virtual brain RL optimizer</li>
            <li>EEG-based OSA risk models</li>
          </ul>
        </div>

        <div class="ontology-node">
          <div class="ontology-title">Workflows</div>
          <ul>
            <li>Evidence-grounded intent classification with schema validation</li>
            <li>Adaptive social-skill training scenarios</li>
            <li>Personalized neuromodulation parameter search</li>
            <li>Treatment planning & biomarker monitoring flows</li>
          </ul>
        </div>

        <div class="ontology-node">
          <div class="ontology-title">Outcomes</div>
          <ul>
            <li>Intent accuracy +27%p, errors ‚àí62.6%</li>
            <li>Reduced manual analysis for educators & clinicians</li>
            <li>3.6√ó faster simulation with improved similarity</li>
            <li>Explainable EEG features supporting decisions</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- WORK -->
  <section id="work" class="section">
    <h2>Operational Impact</h2>

    <!-- PROJECT 1 -->
    <div class="project-block">
      <h3>LLM + RAG Intent Analysis Pipeline</h3>
      <div class="project-meta">LangChain ¬∑ LangGraph ¬∑ FAISS ¬∑ Pydantic</div>

      <div class="figure-grid">
        <div class="figure-item">
          <img src="images/rag-intent-f1-workflow.png" alt="Intent inference workflow" />
          <div class="figure-caption">Figure 1. Intent inference workflow</div>
        </div>
        <div class="figure-item">
          <img src="images/rag-intent-f2-labels.png" alt="Korean intent dataset and label distribution" />
          <div class="figure-caption">Figure 2. Korean utterance samples and label distribution</div>
        </div>
        <div class="figure-item">
          <img src="images/rag-intent-f3-performance.png" alt="Performance comparison with and without RAG" />
          <div class="figure-caption">Figure 3. RAG-based improvement over baseline ML and vanilla LLM</div>
        </div>
      </div>

      <strong>Problem</strong>
      <ul>
        <li>
          User logs were noisy, ambiguous, and inconsistently labeled, making intent classification 
          unreliable for both analytics and downstream automation.
        </li>
      </ul>

      <strong>Data & Structuring</strong>
      <ul>
        <li>
          Consolidated multi-source text logs and legacy labels into a cleaner schema of behavior-based intents.
        </li>
        <li>
          Defined canonical fields (scenario, target user type, evidence span, confidence) to standardize outputs.
        </li>
      </ul>

      <strong>Solution Design</strong>
      <ul>
        <li>
          Built a FAISS-backed RAG pipeline to surface past, high-signal examples as evidence for each prediction.
        </li>
        <li>
          Used LangGraph to orchestrate retrieval, LLM reasoning, and validation steps.
        </li>
        <li>
          Enforced response structure with Pydantic schema validation and retry logic for malformed outputs.
        </li>
      </ul>

      <strong>Impact</strong>
      <ul>
        <li>Accuracy improved from <b>57% ‚Üí 84%</b>.</li>
        <li>Reasoning errors reduced by <b>62.6%</b>, with RAG-only errors down by 14.1%.</li>
        <li>Parsing stability reached <b>99%</b>, with no unsupported, evidence-free answers.</li>
      </ul>
    </div>

    <!-- PROJECT 2 -->
    <div class="project-block">
      <h3>Behavioral Interaction Profiling (ASD)</h3>
      <div class="project-meta">KMeans ¬∑ UMAP ¬∑ LLM Prompt Templates</div>

      

      <div class="figure-grid">

      <div class="figure-item">
        <img src="images/asd-f1-eda.png" alt="EDA of ASD interaction metrics" />
        <div class="figure-caption">Figure 1. EDA of interaction metrics.</div>
      </div>

      <div class="figure-item">
        <img src="images/asd-f2-cluster.png" alt="Cluster visualization using KMeans & UMAP" />
        <div class="figure-caption">Figure 2. Cluster visualization (KMeans + UMAP).</div>
      </div>

  <div class="figure-item">
    <img src="images/asd-f3-scenario.png" alt="Scenario mapping templates" />
    <div class="figure-caption">Figure 3. Scenario templates mapped to clusters.</div>
  </div>

</div>


      <strong>Problem</strong>
      <ul>
        <li>
          ASD learner behavior varied widely, making it hard for educators to design 
          consistent, personalized social-skill training scenarios.
        </li>
      </ul>

      <strong>Data & Structuring</strong>
      <ul>
        <li>
          Standardized dialogue and interaction logs into comparable metrics 
          (turn-taking ratio, utterance length, question/command balance).
        </li>
        <li>
          Aggregated these into learner-level profiles suitable for clustering and downstream scenario mapping.
        </li>
      </ul>

      <strong>Solution Design</strong>
      <ul>
        <li>
          Applied KMeans + UMAP to derive interpretable clusters of interaction styles.
        </li>
        <li>
          Evaluated internal consistency using silhouette score and Davies‚ÄìBouldin index.
        </li>
        <li>
          For each cluster, defined conversational templates and LLM prompt rules 
          to generate tailored practice scenarios.
        </li>
      </ul>

      <strong>Impact</strong>
      <ul>
        <li>
          Produced profiles that summarized key traits, cautions, and recommended feedback strategies.
        </li>
        <li>
          Formed the core module for an ASD social-skills education concept, including flow diagrams and demo scripts.
        </li>
      </ul>
    </div>

    <!-- PROJECT 3 -->
    <div class="project-block">
      <h3>Virtual Brain Optimization for Neuromodulation</h3>
      <div class="project-meta">RL (DDPG) ¬∑ TensorFlow ¬∑ OHBM 2024</div>

      <div class="figure-grid">
        <div class="figure-item">
          <img src="images/virtual-brain-f1-flow.png" alt="Overall research pipeline" />
          <div class="figure-caption">Figure 1. Overall research pipeline and data flow</div>
        </div>
        <div class="figure-item">
          <img src="images/virtual-brain-f2-connectivity.png" alt="Virtual brain connectivity visualization" />
          <div class="figure-caption">Figure 2. Virtual brain connectivity and multimodal integration</div>
        </div>
        <div class="figure-item">
          <img src="images/virtual-brain-f3-ddpg.png" alt="DDPG model architecture" />
          <div class="figure-caption">Figure 3. DDPG model structure for parameter optimization</div>
        </div>
      </div>

      <strong>Problem</strong>
      <ul>
        <li>
          Personalized brain stimulation required slow, expensive simulations and manual parameter sweeps, 
          limiting how many candidates could be reasonably explored.
        </li>
      </ul>

      <strong>Data & Structuring</strong>
      <ul>
        <li>
          Integrated fMRI, sMRI, and DTI into individualized virtual brain models with node-level physiological parameters.
        </li>
        <li>
          Defined a parameter space of 6 variables per node, representing excitatory/inhibitory balances and time constants.
        </li>
      </ul>

      <strong>Solution Design</strong>
      <ul>
        <li>
          Designed a DDPG reinforcement learning pipeline to search the high-dimensional parameter space efficiently.
        </li>
        <li>
          Used cosine similarity between simulated and empirical neural signals as the optimization objective.
        </li>
      </ul>

      <strong>Impact</strong>
      <ul>
        <li>Improved simulation similarity by <b>6.2%</b> on average (up to 15.8% for some subjects).</li>
        <li>Reduced runtime by <b>3.6√ó</b> compared to grid search.</li>
        <li>Results presented as a poster at <b>OHBM 2024</b>.</li>
      </ul>
    </div>

    <!-- PROJECT 4 -->
    <div class="project-block">
      <h3>EEG-based OSA Prediction (XAI)</h3>
      <div class="project-meta">XGBoost ¬∑ RF ¬∑ SVR ¬∑ SHAP</div>

      <div class="figure-grid">
        <div class="figure-item">
          <img src="images/eeg-f1-flow.png" alt="EEG preprocessing and prediction workflow" />
          <div class="figure-caption">
            Figure 1. EEG preprocessing and prediction workflow (AHI / SpO‚ÇÇ).
          </div>
        </div>

        <div class="figure-item">
          <img src="images/eeg-f2-signal.png" alt="EEG raw signal and engineered features" />
          <div class="figure-caption">
            Figure 2. EEG raw signals and engineered feature extraction per channel.
          </div>
        </div>

        <div class="figure-item">
          <img src="images/eeg-f3-models.png" alt="Model comparison with RF, XGBoost, SVR" />
          <div class="figure-caption">
            Figure 3. Model comparison across RF, XGBoost, and SVR for AHI/SpO‚ÇÇ prediction.
          </div>
        </div>

        <div class="figure-item">
          <img src="images/eeg-f4-shap.png" alt="SHAP-based feature importance" />
          <div class="figure-caption">
            Figure 4. SHAP-based feature and channel importance for clinical interpretation.
          </div>
        </div>
      </div>

      <strong>Problem</strong>
      <ul>
        <li>
          EEG was high-dimensional and noisy, making it difficult to personalize OSA severity assessment 
          and CPAP pressure decisions.
        </li>
      </ul>

      <strong>Data & Structuring</strong>
      <ul>
        <li>
          Engineered 29 EEG features per channel and standardized them into patient-level feature sets.
        </li>
        <li>
          Framed prediction tasks for AHI and SpO‚ÇÇ so they aligned with how clinicians interpret severity.
        </li>
      </ul>

      <strong>Solution Design</strong>
      <ul>
        <li>
          Trained and compared multiple ML models (RF, XGBoost, SVR) on the engineered feature space.
        </li>
        <li>
          Applied SHAP to highlight influential features and channels in a clinician-friendly format.
        </li>
      </ul>

      <strong>Impact</strong>
      <ul>
        <li>Achieved <b>85‚Äì88%</b> prediction accuracy for AHI and SpO‚ÇÇ.</li>
        <li>
          Proposed a personalized CPAP pressure decision-support flow grounded in interpretable EEG biomarkers.
        </li>
      </ul>
    </div>

  </section>

</div>

<footer>
  ¬© 2025 Juhyeong Kang
</footer>

</body>
</html>
